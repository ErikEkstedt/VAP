<!DOCTYPE html>
<html lang="en">
  <head>
    <title>VAP</title>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <script src="https://cdn.tailwindcss.com"></script>
  </head>

  <body class="bg-slate-200">
    <nav class="bg-gray-800 text-white py-3 shadow-md">
      <div
        class="max-w-screen-xl mx-auto px-4 sm:px-6 lg:px-8 flex items-center justify-between"
      >
        <!-- Brand/Logo -->
        <a
          href="#"
          class="text-2xl font-bold tracking-wider hover:text-gray-400 transition duration-150 ease-in-out"
          >VAP</a
        >

        <!-- Nav Items for Large Screens -->
        <div class="hidden sm:flex space-x-6">
          <a
            href="#model"
            class="hover:text-gray-400 transition duration-150 ease-in-out focus:outline-none focus:text-gray-400"
            >Model</a
          >
          <a
            href="#demo"
            class="hover:text-gray-400 transition duration-150 ease-in-out focus:outline-none focus:text-gray-400"
            >Demo</a
          >
          <a
            href="#projection-window"
            class="hover:text-gray-400 transition duration-150 ease-in-out focus:outline-none focus:text-gray-400"
            >Projection Window</a
          >
          <a
            href="#events"
            class="hover:text-gray-400 transition duration-150 ease-in-out focus:outline-none focus:text-gray-400"
            >Events</a
          >
          <a
            href="#zero-shot"
            class="hover:text-gray-400 transition duration-150 ease-in-out focus:outline-none focus:text-gray-400"
            >Zero-shot</a
          >
          <a
            href="#contact"
            class="hover:text-gray-400 transition duration-150 ease-in-out focus:outline-none focus:text-gray-400"
            >Contact</a
          >
        </div>
      </div>
    </nav>

    <!-- Jumbotron -->
    <div class="px-5 bg-white rounded-lg">
      <div class="max-w-screen-xl mx-auto py-5 px-4 sm:px-6 lg:px-8">
        <div
          class="flex flex-col md:flex-row items-start space-y-5 md:space-y-0 md:space-x-5"
        >
          <!-- Summary Text -->
          <div class="px-2 md:w-3/5">
            <h1 class="text-2xl font-bold">Voice Activity Projection, VAP</h1>
            <p class="mt-4 text-md">
              A general, incremental and predictive model of conversational
              dynamics. The model process the voice activity and audio waveform
              of spoken dialog and outputs a probability distribution over
              <i>projection-windows</i> (discrete states) that represents
              speaker activity over the next 2 seconds. The main purpose is to
              use it as a <strong>turn-taking</strong> model for spoken dialog
              systems.
            </p>

            <div class="mt-4">
              <h1 class="text-xl font-bold leading-tight">Papers</h1>
              <ul class="list-disc list-outside text-xs pl-4">
                <li class="p-1">
                  <a
                    class="underline hover:text-gray-500"
                    href="https://www.isca-speech.org/archive/interspeech_2022/ekstedt22_interspeech.html"
                  >
                    Voice Activity Projection: Self-supervised Learning of
                    Turn-taking Events
                  </a>
                  <ul class="list-disc list-inside">
                    <li>E. Ekstedt and G. Skantze, INTERSPEECH 2022</li>
                  </ul>
                </li>
                <li class="p-1">
                  <a
                    class="underline hover:text-gray-500"
                    href="https://aclanthology.org/2022.sigdial-1.51/"
                  >
                    How Much Does Prosody Help Turn-taking? Investigations using
                    Voice Activity Projection Models
                  </a>
                  <ul class="list-disc list-inside text-xs">
                    <li>
                      <span class="text-red-400 m-0 p-0"
                        >Best Paper Award Winner
                      </span>
                    </li>
                    <li>E. Ekstedt and G. Skantze, SIGDIAL 2022</li>
                  </ul>
                </li>
                <li class="p-1">
                  <a class="underline hover:text-gray-500" href="#">
                    What makes a good pause? Investigating the turn-holding
                    effects of fillers
                  </a>
                  <ul class="list-disc list-inside text-xs">
                    <li>B. Jang, E. Ekstedt and G. Skantze, ICPhs 2023</li>
                  </ul>
                </li>
                <li class="p-1">
                  <a class="underline hover:text-gray-500" href="#">
                    Automatic Evaluation of Turn-taking Cues in Conversational
                    Speech Synthesis
                  </a>
                  <ul class="list-disc list-inside text-xs">
                    <li>
                      E. Ekstedt, S. Wang, É. Székely, J. Gustafson and G.
                      Skantze, INTERSPEECH 2023
                    </li>
                  </ul>
                </li>
              </ul>
            </div>
          </div>

          <!-- List Items -->
          <div class="grid grid-cols-1 md:grid-cols-2 md:w-2/5 gap-6 p-8">
            <figure>
              <img
                src="./assets/imgs/vap.png"
                class="max-w-full h-auto bg-white"
                alt="VAP Model"
              />
              <figcaption class="text-xs text-center mt-2">
                VAP Model
              </figcaption>
            </figure>
            <figure>
              <img
                src="./assets/imgs/VapStereo.png"
                class="max-w-full h-auto bg-white"
                alt="VAP stereo Model"
              />
              <figcaption class="text-xs text-center mt-2">
                VAP Stereo Model
              </figcaption>
            </figure>
            <figure>
              <img
                class="inline-block"
                src="./assets/imgs/output/sl_question.png"
                alt="SL Question"
              />
              <figcaption class="text-xs text-center mt-2">
                SIGDial22
              </figcaption>
            </figure>
            <figure>
              <img
                class="inline-block"
                src="./assets/imgs/output/filler.png"
                alt="Filler"
              />
              <figcaption class="text-xs text-center mt-2">ICPhS23</figcaption>
            </figure>
            <figure class="col-span-2">
              <img
                class="inline-block w-full"
                src="./assets/imgs/output/tts.png"
                alt="TTS"
              />
              <figcaption class="text-xs text-center mt-2">
                Interspeech23
              </figcaption>
            </figure>
            <figure class="col-span-2 flex">
              <div>
                <img
                  class="inline-block"
                  src="./assets/imgs/output/turn_end.png"
                  alt="Turn End"
                />
                <figcaption class="text-xs text-center mt-2">
                  Turn End
                </figcaption>
              </div>
              <div>
                <img
                  class="inline-block"
                  src="./assets/imgs/output/bc_pred.png"
                  alt="BC Prediction"
                />
                <figcaption class="text-xs text-center mt-2">
                  BC Prediction
                </figcaption>
              </div>
            </figure>
          </div>
        </div>
      </div>
    </div>

    <!-- Content -->
    <div class="w-full">
      <section id="model" class="bg-slate-200 pt-5 px-5">
        <h2 class="text-xl font-bold mb-4">Model</h2>
        <div class="container mx-auto px-6 py-4">
          <h2 class="text-md font-bold">Voice Activity Projection (VAP)</h2>
          <blockquote class="max-w-2xl my-4">
            <p class="italic text-sm mb-2">
              "VAP is a general, incremental, and predictive model of
              conversational dynamics. It's akin to a 'Language Model' that
              outputs a probability distribution over projection-windows in a
              dialog."
            </p>
          </blockquote>

          <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
            <!-- Model Description -->
            <div>
              <h4 class="text-xl font-medium mb-2">Model Description</h4>
              <p class="text-sm leading-relaxed">
                The VAP model integrates an encoder that processes raw audio
                waveforms and the prevailing VA data to generate latent frame
                frequency representations. These are then supplied to the
                predictor network. Acting as a causal sequence network, the
                predictor processes the context up to the current frame,
                resulting in a probability distribution across 256
                <i>projection-window</i> states.
              </p>

              <div class="text-sm">
                <h4 class="text-md font-medium mt-2">Main Strengths</h4>
                <ul class="list-disc text-xs pl-6">
                  <li class="font-medium">General</li>
                  <ul class="list-disc pl-4">
                    <li>No annotations</li>
                    <li>No feature extraction</li>
                    <li>No speaker normalization</li>
                  </ul>
                  <li class="font-medium mt-2">Incremental</li>
                  <ul class="list-disc pl-4">
                    <li>Train, evaluate, and utilize across entire dialogs</li>
                  </ul>
                  <li class="font-medium mt-2">Predictive</li>
                  <ul class="list-disc pl-4">
                    <li>Predict upcoming activity</li>
                    <li>Allocate extra processing time for speech</li>
                  </ul>
                </ul>
              </div>
            </div>
            <div class="flex items-center w-full">
              <img
                src="./assets/imgs/vap.png"
                class="max-w-full h-auto bg-white"
                alt="model image"
              />
            </div>
          </div>

          <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mt-4">
            <div>
              <!-- Transformer Section -->
              <h3 class="text-xl font-medium mb-2">Transformer</h3>
              <ul class="list-disc text-xs pl-6 mb-6">
                <li>GPT-like, decoder-only, unidirectional</li>
                <li>4 layers</li>
                <li>4 heads</li>
                <li>256 dim</li>
                <li>AliBi attention</li>
                <ul class="list-disc pl-4 space-y-1">
                  <li>
                    <a
                      href="https://ofir.io/train_short_test_long.pdf"
                      class="text-blue-600 hover:underline"
                      >Paper</a
                    >
                  </li>
                  <li>
                    <a
                      href="https://github.com/ofirpress/attention_with_linear_biases"
                      class="text-blue-600 hover:underline"
                      >Github</a
                    >
                  </li>
                </ul>
                <li>
                  <a
                    href="https://arxiv.org/pdf/1706.03762.pdf"
                    class="text-blue-600 hover:underline"
                  >
                    Attention is all you need (Vaswani et al, 2017)
                  </a>
                </li>
              </ul>
            </div>
            <div>
              <!-- CPC Section -->
              <h3 class="text-xl font-medium mb-2">CPC</h3>
              <ul class="list-disc text-xs pl-6 mb-6">
                <li>CNN-1D</li>
                <ul class="list-disc pl-4 space-y-1">
                  <li>5 layers</li>
                  <li>256 dim</li>
                  <li>Sequential</li>
                </ul>
                <li>LSTM</li>
                <ul class="list-disc pl-4 space-y-1">
                  <li>1 layer</li>
                  <li>256 dim</li>
                </ul>
                <li>Papers</li>
                <ul class="list-disc pl-4 space-y-1">
                  <li>
                    <a
                      href="https://arxiv.org/pdf/1807.03748.pdf"
                      class="text-blue-600 hover:underline"
                    >
                      Representation Learning with Contrastive Predictive
                      Coding, (Oord et al, 2018)
                    </a>
                  </li>
                  <li>
                    <a
                      href="https://arxiv.org/pdf/2002.02848.pdf"
                      class="text-blue-600 hover:underline"
                    >
                      Unsupervised Pretraining Transfers Well across Languages,
                      (Riviere et al, 2020)
                    </a>
                  </li>
                  <li>
                    <a
                      href="https://github.com/facebookresearch/CPC_audio/tree/main/cpc"
                      class="text-blue-600 hover:underline"
                    >
                      Code & pretrained weights
                    </a>
                  </li>
                </ul>
              </ul>
            </div>
          </div>
        </div>
      </section>

      <section id="demo" class="bg-slate-100 pt-5 px-5">
        <h2 class="text-xl font-bold mb-4">Demo</h2>
        <div class="flex px-4">
          <!-- Image -->
          <div class="flex-none w-1/4">
            <img
              class="w-full h-auto"
              src="./assets/imgs/demo.png"
              alt="Demo Image"
            />
          </div>

          <!-- Video -->
          <div class="flex-grow w-3/4 pl-4">
            <!-- Added pl-4 for spacing between image and video -->
            <video class="w-full h-auto" controls>
              <source src="./assets/HerVap.mp4" type="video/mp4" />
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </section>

      <!-- Projection Window -->
      <section id="projection-window" class="px-5 pt-5 bg-slate-200">
        <h2 class="text-xl font-bold mb-4">Projection Window</h2>
        <div class="flex flex-wrap md:flex-nowrap">
          <!-- Text Section -->
          <div class="flex-1 pr-6">
            <figure class="text-right">
              <blockquote
                class="border-l-4 border-gray-300 pl-4 italic text-gray-600"
              >
                <p class="mb-4">
                  "The calibration of conversational timing and the capacity for
                  projecting when others are going to stop speaking are crucial
                  elements of the conversational machine."
                </p>
              </blockquote>
              <figcaption class="text-sm font-light">
                N.J.Enfield's book
                <cite class="italic">
                  How we Talk. Chapter 4, The One Second Window.
                </cite>
              </figcaption>
            </figure>
            <div class="mt-6 space-y-4 text-left">
              <p>
                We want to model the future voice activity in order to predict
                turn-taking decisions over dyadic spoken dialog. The
                representation of the future should be short enough to introduce
                minimal noise whilst long enough to predict turn-shifts in
                advance and to discern shifts from shorter segments of activity
                (backchannels).
              </p>
              <p>
                The projection window consists of voice activity over 2 seconds
                of dialog. We divide the window into 4 regions of increasing
                duration 200ms, 400ms, 600ms and 800ms to create 8 bins. The
                ratio of activity is calculated over each bin and is considered
                active above 50% to create a discrete onehot representation of
                size (2, 4). The vector is mapped to an index in a
                codebook/vocabulary by treating it as a binary number.
              </p>
            </div>
          </div>

          <!-- Image Section -->
          <div class="flex-shrink-0 flex-1 pt-6 md:pt-0">
            <img
              class="w-full mb-4 bg-white"
              src="./assets/imgs/projwin2.png"
              alt="projection window"
            />
            <img
              class="w-full mx-auto bg-white"
              src="./assets/imgs/projwin.png"
              alt="projection window"
            />
          </div>
        </div>
      </section>

      <!-- Events -->
      <section id="events" class="pt-5 px-5 bg-slate-100">
        <h2 class="text-xl font-bold mb-4">Events</h2>
        <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
          <!-- Shift vs Hold Column -->
          <div class="p-2">
            <h3 class="text-xl font-bold mb-4">Shift vs Hold</h3>
            <div class="flex flex-col items-center justify-center">
              <img
                class="max-w-[500px] mb-4 bg-white"
                src="./assets/imgs/ShiftHold.png"
                alt="Shift vs Hold Visualization"
              />
            </div>
            <div class="text-sm px-4">
              <p class="text-sm font-medium italic">
                Predict the next speaker during
                <strong>mutual silences</strong>.
              </p>
              <p class="mt-2">
                We distinguish two regions: pre-offset and post-offset, ensuring
                only one speaker is active to prevent ambiguous overlap moments.
                Determining the last and next speaker helps identify a Shift or
                a Hold.
              </p>
            </div>
          </div>

          <!-- Shift-Prediction Column -->
          <div class="p-2">
            <h3 class="text-xl font-bold mb-4">Shift-Prediction</h3>
            <div class="flex flex-col items-center justify-center">
              <img
                class="max-w-[500px] mb-4 bg-white"
                src="./assets/imgs/ShiftPred.png"
                alt="Shift Prediction Visualization"
              />
              <img
                class="max-w-[500px] mb-4 bg-white"
                src="./assets/imgs/ShiftPredNeg.png"
                alt="Negative Shift Prediction Visualization"
              />
            </div>

            <div class="text-sm px-4">
              <p class="font-medium italic">
                Predict the next speaker during <strong>active speech</strong>.
              </p>
              <p class="mt-2">
                We establish a 0.5s prediction region over the last activity
                segment prior to a Shift event. Positive shift-predictions are
                identified from this. Negatives are sourced from a speaker's
                active regions distant (2s) from the listener's activity.
              </p>
            </div>
          </div>

          <!-- BC-Prediction Column -->
          <div class="p-2">
            <h3 class="text-xl font-bold mb-4">BC-Prediction</h3>

            <div class="flex flex-col items-center justify-center">
              <img
                class="max-w-[500px] mb-4 bg-white"
                src="./assets/imgs/bc.png"
                alt="Backchannel Prediction Visualization"
              />
              <img
                class="max-w-[500px] mb-4 bg-white"
                src="./assets/imgs/BCPred.png"
                alt="BC Prediction Visualization"
              />
            </div>

            <div class="text-sm px-4">
              <p class="font-medium italic">
                Predict upcoming backchannels (<strong>BC</strong>s).
              </p>
              <p class="mt-2">
                A backchannel (BC) is a brief listener activity indicating
                attention or acknowledgement, such as "oh", "yeah", or "mhm".
              </p>
              <p class="mt-2">
                BCs are identified as short, isolated listener activity
                segments. They must be brief (less than 1s) and isolated, marked
                by 1s pre-silence and 2s post-silence.
              </p>
            </div>
          </div>

          <!-- Short vs Long Column -->
          <div class="p-2">
            <h3 class="text-xl font-bold mb-4">Short vs Long</h3>

            <div class="flex flex-col items-center justify-center">
              <img
                class="max-w-[500px] mb-4 bg-white"
                src="./assets/imgs/ShortLong.png"
                alt="Short vs Long Visualization"
              />
            </div>
            <div class="text-sm px-4">
              <p class="italic font-medium">
                Determine utterance length (Short vs Long) at its onset during a
                shift event.
              </p>
              <p class="mt-2">
                All BC events are labeled Short while all Shift events are
                considered Long. The prediction zone covers the initial 0.2s of
                the utterance. This helps decide if a user's activity during the
                system's speech should lead to interruption or if the system can
                proceed.
              </p>
            </div>
          </div>
        </div>
      </section>

      <section id="zero-shot" class="pt-5 px-5 bg-slate-200">
        <h2 class="text-xl font-bold mb-4">Zero-shot Evaluation</h2>
        <div class="container text-sm mx-auto px-6 py-4">
          <p class="italic mb-2">
            How to predict turn-taking actions from a general distribution over
            future voice activity?
          </p>
          <p class="mb-4">
            All zero-shot methods follow a similar approach where researchers
            select subsets over the total states which correspond to a clear
            outcome. For instance, during a mutual silence, we want to find the
            most likely next speaker. We choose subsets where either only
            speaker A is active or only speaker B is active. By defining these
            two subsets, we compare their probabilities and determine the most
            likely outcome.
          </p>
          <p>
            The examples below primarily visualize the state for a particular
            listener (blue) and the current speaker (yellow), but it's
            symmetrically accurate in reverse.
          </p>

          <div class="grid grid-cols-1 md:grid-cols-2 gap-4 mt-4">
            <!-- Shift vs Hold -->
            <div class="p-2">
              <h3 class="text-xl font-medium mb-2">Shift vs Hold</h3>
              <img
                class="max-w-[500px] w-full mx-auto mb-2 bg-white"
                src="./assets/imgs/zeroshot.png"
                alt="Shift vs Hold Visualization"
              />
              <div class="px-2">
                <p class="font-medium mb-2 italic">
                  Predict the next speaker during
                  <strong>mutual silence</strong>.
                </p>
                <p>
                  We select symmetrical subsets where only one speaker is
                  active, ensuring that the last two bins are active. The final
                  bins cover a 1.2s duration and signify the next turn's
                  probable speaker. Given the silence, there's uncertainty when
                  the activity starts, so the first two bins might also be
                  active. We define four subsets for each speaker as illustrated
                  above.
                </p>
              </div>
            </div>

            <!-- Shift-Prediction -->
            <div class="p-2">
              <h3 class="text-xl font-medium mb-2">Shift-Prediction</h3>
              <div class="flex justify-around mb-2">
                <img
                  class="w-2/5 border p-2 bg-white"
                  src="./assets/imgs/predshift.png"
                  alt="Shift Prediction Visualization"
                />
                <img
                  class="w-2/5 bg-white"
                  src="./assets/imgs/predshift2.png"
                  alt="Shift Prediction Visualization 2"
                />
              </div>
              <div class="px-2">
                <p class="font-medium mb-2 italic">
                  Predict the next speaker during
                  <strong>active speech</strong>.
                </p>
                <p>
                  Shift-prediction parallels "Shift vs Hold" but includes
                  activity in the current speaker's initial two bins. Given the
                  active region, the uncertainty lies in determining the
                  utterance's end. Therefore, we incorporate all states where
                  the current speaker's initial two bins indicate activity. This
                  method means we compare the listener's 12 states to the
                  current speaker's 4 states (from Shift vs Hold) for predicting
                  an upcoming turn-shift.
                </p>
              </div>
            </div>

            <!-- BC-Prediction -->
            <div class="p-2">
              <h3 class="text-xl font-medium mb-2">BC-Prediction</h3>
              <img
                class="w-3/5 mx-auto mb-2 bg-white"
                src="./assets/imgs/zero_bc_pred.png"
                alt="BC Prediction Visualization"
              />
              <div class="px-2">
                <p class="font-medium mb-2 italic">
                  Predict an upcoming backchannel during active speech or mutual
                  silences.
                </p>
                <p>
                  The BC-prediction subset represents a short listener activity
                  segment and the current speaker continuing their turn. We
                  define two constraints: the listener must have at least one
                  active bin from the starting three, and the current speaker's
                  last two bins must be active. During evaluations, we set a
                  threshold to convert probabilities into a discrete action.
                </p>
              </div>
            </div>

            <!-- Short vs Long -->
            <div class="p-2">
              <h3 class="text-xl font-medium mb-2">Short vs Long</h3>
              <img
                class="w-3/5 mx-auto mb-2 bg-white"
                src="./assets/imgs/zero_bc_pred.png"
                alt="Short vs Long Prediction Visualization"
              />
              <div class="p-2">
                <p class="font-medium mb-2 italic">
                  Predict if a recent onset belongs to a Long or Short activity
                  segment.
                </p>
                <p class="mb-4">
                  This prediction utilizes the subsets defined in BC-prediction
                  but evaluates them at an activity segment's onset, with the
                  speaker roles reversed. <strong>PS.</strong> The
                  "bc-probabilities" are defined for both speakers and can
                  represent a backchannel prediction when the speaker is
                  listening or an end of activity indication if they are active.
                  <strong>DS.</strong>
                </p>
              </div>
            </div>
          </div>
        </div>
      </section>
    </div>

    <footer id="contact" class="bg-white shadow mt-4 dark:bg-gray-800">
      <div
        class="w-full mx-auto max-w-screen-xl p-4 md:flex md:items-center md:justify-between"
      >
        <span class="text-sm text-gray-500 sm:text-center dark:text-gray-400">
          <a href="#" class="mr-4 hover:underline md:mr-6"> Contact </a>
        </span>
        <ul
          class="flex flex-wrap items-center mt-3 text-sm font-medium text-gray-500 dark:text-gray-400 sm:mt-0"
        >
          <li>
            <a
              href="mailto: erikekst@kth.se"
              class="mr-4 hover:underline md:mr-6"
            >
              erikekst@kth.se</a
            >
          </li>
          <li>
            <a
              href="mailto: skantze@kth.se"
              class="mr-4 hover:underline md:mr-6"
            >
              skantze@kth.se</a
            >
          </li>
        </ul>
      </div>
    </footer>
  </body>
</html>
